{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from attentional_cpmp.model import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method StackAttention.call of <attentional_cpmp.layers.StackAttention.StackAttention object at 0x0000012A1D80A2D0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method StackAttention.call of <attentional_cpmp.layers.StackAttention.StackAttention object at 0x0000012A1D80A2D0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeedForward.call of <attentional_cpmp.layers.FeedForward.FeedForward object at 0x0000012A1D80BB90>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeedForward.call of <attentional_cpmp.layers.FeedForward.FeedForward object at 0x0000012A1D80BB90>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x0000012A1D805A50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x0000012A1D805A50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 7\n",
    "key_dim = H + 1\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attentional_cpmp.utils.data_saving.data_json import load_data_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Sx7 = load_data_from_json(file_path=\"CPMP_With_Attention.Sx7_v4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "claves_a_incluir = ['5', '6', '7']\n",
    "\n",
    "# Crear subdiccionario\n",
    "sub_data_Sx7 = dict((clave, data_Sx7[clave]) for clave in claves_a_incluir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['5', '6', '7'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data_Sx7.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1973/1973 [==============================] - 96s 34ms/step - loss: 0.1851 - mae: 0.1101 - mse: 0.0548\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 64s 32ms/step - loss: 0.1676 - mae: 0.1010 - mse: 0.0503\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 49s 25ms/step - loss: 0.1646 - mae: 0.0990 - mse: 0.0493\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 43s 22ms/step - loss: 0.1628 - mae: 0.0980 - mse: 0.0488\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 43s 22ms/step - loss: 0.1617 - mae: 0.0972 - mse: 0.0485\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1609 - mae: 0.0967 - mse: 0.0483\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 44s 22ms/step - loss: 0.1599 - mae: 0.0961 - mse: 0.0480\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 39s 20ms/step - loss: 0.1592 - mae: 0.0957 - mse: 0.0478\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1585 - mae: 0.0952 - mse: 0.0475\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1580 - mae: 0.0949 - mse: 0.0474\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 71s 27ms/step - loss: 0.1381 - mae: 0.0812 - mse: 0.0406\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 65s 28ms/step - loss: 0.1370 - mae: 0.0807 - mse: 0.0403\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 61s 26ms/step - loss: 0.1366 - mae: 0.0805 - mse: 0.0402\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1360 - mae: 0.0801 - mse: 0.0401\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 64s 28ms/step - loss: 0.1356 - mae: 0.0799 - mse: 0.0400\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 62s 27ms/step - loss: 0.1353 - mae: 0.0797 - mse: 0.0399\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1350 - mae: 0.0795 - mse: 0.0398\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 64s 27ms/step - loss: 0.1347 - mae: 0.0794 - mse: 0.0397\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1344 - mae: 0.0792 - mse: 0.0396\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 61s 26ms/step - loss: 0.1341 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1180 - mae: 0.0676 - mse: 0.0338\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1175 - mae: 0.0674 - mse: 0.0337\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 88s 35ms/step - loss: 0.1170 - mae: 0.0672 - mse: 0.0336\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 78s 31ms/step - loss: 0.1167 - mae: 0.0670 - mse: 0.0335\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1165 - mae: 0.0669 - mse: 0.0334\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 78s 31ms/step - loss: 0.1162 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1159 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 78s 31ms/step - loss: 0.1158 - mae: 0.0665 - mse: 0.0333\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 78s 31ms/step - loss: 0.1155 - mae: 0.0664 - mse: 0.0332\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 78s 30ms/step - loss: 0.1154 - mae: 0.0663 - mse: 0.0332\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 46.1\n",
      "mean steps: 57.67245119305857\n",
      "median steps: 58.0\n",
      "min steps: 40.0\n",
      "max steps: 86.0\n",
      "\n",
      "success heuristic (%): 97.1 55.53038105046344\n",
      "mean steps: 55.53038105046344\n",
      "median steps: 55.0\n",
      "min steps: 36.0\n",
      "max steps: 79.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46.1, 97.1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 57s 21ms/step - loss: 0.1873 - mae: 0.1104 - mse: 0.0547\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1651 - mae: 0.0991 - mse: 0.0494\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1621 - mae: 0.0973 - mse: 0.0485\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 46s 23ms/step - loss: 0.1606 - mae: 0.0964 - mse: 0.0481\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 39s 20ms/step - loss: 0.1595 - mae: 0.0958 - mse: 0.0478\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1587 - mae: 0.0953 - mse: 0.0476\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1581 - mae: 0.0950 - mse: 0.0474\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 44s 22ms/step - loss: 0.1577 - mae: 0.0948 - mse: 0.0473\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 45s 23ms/step - loss: 0.1572 - mae: 0.0944 - mse: 0.0472\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 39s 20ms/step - loss: 0.1568 - mae: 0.0942 - mse: 0.0470\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 70s 27ms/step - loss: 0.1372 - mae: 0.0807 - mse: 0.0404\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 64s 27ms/step - loss: 0.1363 - mae: 0.0802 - mse: 0.0401\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1357 - mae: 0.0799 - mse: 0.0400\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1353 - mae: 0.0796 - mse: 0.0398\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 62s 27ms/step - loss: 0.1349 - mae: 0.0795 - mse: 0.0398\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1346 - mae: 0.0793 - mse: 0.0397\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 62s 27ms/step - loss: 0.1343 - mae: 0.0792 - mse: 0.0396\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 65s 28ms/step - loss: 0.1341 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 66s 29ms/step - loss: 0.1338 - mae: 0.0789 - mse: 0.0394\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 60s 26ms/step - loss: 0.1335 - mae: 0.0788 - mse: 0.0394\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 89s 35ms/step - loss: 0.1175 - mae: 0.0674 - mse: 0.0337\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1170 - mae: 0.0671 - mse: 0.0336\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1167 - mae: 0.0669 - mse: 0.0335\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1164 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 78s 31ms/step - loss: 0.1162 - mae: 0.0667 - mse: 0.0334\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1161 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 78s 30ms/step - loss: 0.1159 - mae: 0.0665 - mse: 0.0333\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1157 - mae: 0.0665 - mse: 0.0332\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 78s 30ms/step - loss: 0.1156 - mae: 0.0664 - mse: 0.0332\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1154 - mae: 0.0663 - mse: 0.0331\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_2.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 27.0\n",
      "mean steps: 58.28888888888889\n",
      "median steps: 58.0\n",
      "min steps: 39.0\n",
      "max steps: 78.0\n",
      "\n",
      "success heuristic (%): 97.89999999999999 56.25229826353422\n",
      "mean steps: 56.25229826353422\n",
      "median steps: 56.0\n",
      "min steps: 36.0\n",
      "max steps: 83.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27.0, 97.89999999999999)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model_2, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "key_dim = 32\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_3 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 68s 27ms/step - loss: 0.1866 - mae: 0.1107 - mse: 0.0550\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 56s 29ms/step - loss: 0.1665 - mae: 0.0999 - mse: 0.0499\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 56s 28ms/step - loss: 0.1631 - mae: 0.0979 - mse: 0.0488\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 56s 29ms/step - loss: 0.1613 - mae: 0.0968 - mse: 0.0483\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1604 - mae: 0.0962 - mse: 0.0481\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1592 - mae: 0.0955 - mse: 0.0477\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1585 - mae: 0.0952 - mse: 0.0476\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 58s 29ms/step - loss: 0.1578 - mae: 0.0948 - mse: 0.0474\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 58s 30ms/step - loss: 0.1571 - mae: 0.0944 - mse: 0.0472\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 58s 29ms/step - loss: 0.1567 - mae: 0.0942 - mse: 0.0471\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 92s 36ms/step - loss: 0.1370 - mae: 0.0806 - mse: 0.0403\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 84s 36ms/step - loss: 0.1362 - mae: 0.0802 - mse: 0.0401\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1357 - mae: 0.0799 - mse: 0.0400\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1353 - mae: 0.0797 - mse: 0.0399\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 87s 37ms/step - loss: 0.1349 - mae: 0.0795 - mse: 0.0397\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 82s 35ms/step - loss: 0.1345 - mae: 0.0792 - mse: 0.0396\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 82s 35ms/step - loss: 0.1342 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1339 - mae: 0.0789 - mse: 0.0395\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 82s 35ms/step - loss: 0.1337 - mae: 0.0787 - mse: 0.0394\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1334 - mae: 0.0786 - mse: 0.0393\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 104s 41ms/step - loss: 0.1177 - mae: 0.0674 - mse: 0.0337\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 105s 41ms/step - loss: 0.1172 - mae: 0.0671 - mse: 0.0336\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 103s 40ms/step - loss: 0.1169 - mae: 0.0669 - mse: 0.0335\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 104s 41ms/step - loss: 0.1165 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 99s 39ms/step - loss: 0.1163 - mae: 0.0667 - mse: 0.0334\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 99s 39ms/step - loss: 0.1161 - mae: 0.0665 - mse: 0.0333\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1159 - mae: 0.0665 - mse: 0.0332\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1157 - mae: 0.0663 - mse: 0.0332\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 97s 38ms/step - loss: 0.1156 - mae: 0.0663 - mse: 0.0332\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1154 - mae: 0.0662 - mse: 0.0331\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_3.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 61.9\n",
      "mean steps: 54.919224555735056\n",
      "median steps: 55.0\n",
      "min steps: 39.0\n",
      "max steps: 74.0\n",
      "\n",
      "success heuristic (%): 97.1 55.92584963954686\n",
      "mean steps: 55.92584963954686\n",
      "median steps: 56.0\n",
      "min steps: 34.0\n",
      "max steps: 79.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61.9, 97.1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model_3, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model_1.h5')\n",
    "model_2.save('model_2.h5')\n",
    "model_3.save('model_3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
